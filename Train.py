import pandas as pd
import numpy as np
import joblib
import warnings
import os
import json
import io
import sys
import traceback
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from lightgbm import LGBMClassifier

warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

def run_training_pipeline(model_choice: str, custom_params: dict, input_csv_path: str):
    """
    The main training pipeline function, callable from other scripts like app.py.
    """
    old_stdout = sys.stdout
    sys.stdout = captured_output = io.StringIO()
    
    trained_artifacts = {
        "model": None,
        "imputer": None,
        "scaler": None,
        "feature_names": None
    }
    output_model_path = None
    # ### ä¿®æ”¹é–‹å§‹ ###ï¼šåˆå§‹åŒ–æ··æ·†çŸ©é™£è®Šæ•¸
    confusion_matrix_data = None
    # ### ä¿®æ”¹çµæŸ ###

    try:
        print(f"--- é–‹å§‹ç·šä¸Šè¨“ç·´æµç¨‹ï¼š{model_choice} ---")
        print(f"ä½¿ç”¨è³‡æ–™é›†: {input_csv_path}")
        print(f"è‡ªè¨‚è¶…åƒæ•¸: {json.dumps(custom_params, indent=2)}")

        # --- 1. Data Loading and Preparation ---
        df = pd.read_csv(input_csv_path, dtype={'source_id': str})
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        
        training_df = df[df['disposition'].isin(['CONFIRMED', 'FALSE POSITIVE'])].copy()
        if training_df.empty:
            raise ValueError("æä¾›çš„è³‡æ–™é›†ä¸­æ²’æœ‰ 'CONFIRMED' æˆ– 'FALSE POSITIVE' æ¨™ç±¤çš„è³‡æ–™ã€‚")

        training_df['disposition'] = training_df['disposition'].map({'CONFIRMED': 1, 'FALSE POSITIVE': 0})
        
        if 'source_telescope' in training_df.columns:
            training_df = pd.get_dummies(training_df, columns=['source_telescope'], drop_first=True)
        
        X = training_df.drop(columns=['disposition', 'source_id'])
        y = training_df['disposition']
        feature_names = X.columns.tolist()

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42, stratify=y
        )

        imputer = SimpleImputer(strategy='median')
        X_train_imputed = imputer.fit_transform(X_train)
        X_test_imputed = imputer.transform(X_test)

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train_imputed)
        X_test_scaled = scaler.transform(X_test_imputed)
        print("âœ… è³‡æ–™è¼‰å…¥èˆ‡é è™•ç†å®Œæˆã€‚")

        # --- 2. Model Configuration ---
        MODELS_CONFIG = {
            "LogisticRegression": {"model": LogisticRegression, "params": {"random_state": 42, "class_weight": "balanced", "max_iter": 5000}},
            "RandomForest": {"model": RandomForestClassifier, "params": {"random_state": 42, "class_weight": "balanced"}},
            "GradientBoosting": {"model": GradientBoostingClassifier, "params": {"random_state": 42}},
            "LightGBM": {"model": LGBMClassifier, "params": {"random_state": 42, "verbosity": -1, "class_weight": "balanced"}}
        }

        if model_choice not in MODELS_CONFIG:
            raise ValueError(f"é¸æ“‡çš„æ¨¡å‹ '{model_choice}' ä¸è¢«æ”¯æ´ã€‚")

        model_config = MODELS_CONFIG[model_choice]
        model_config["params"].update(custom_params)
        model = model_config["model"](**model_config["params"])
        print(f"\nâœ… æ¨¡å‹ '{model_choice}' å¯¦ä¾‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æœ€çµ‚åƒæ•¸: {model.get_params()}")

        # --- 3. Training ---
        print("\nğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹...")
        model.fit(X_train_scaled, y_train)
        print("âœ… æ¨¡å‹è¨“ç·´å®Œæˆã€‚")
        
        # --- 4. Evaluation and Reporting ---
        print("\nğŸ“‹ ç”¢ç”Ÿæ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šçš„è¡¨ç¾å ±å‘Š...")
        target_names = ['FALSE POSITIVE', 'CONFIRMED']
        y_pred = model.predict(X_test_scaled)
        report = classification_report(y_test, y_pred, target_names=target_names, output_dict=False)
        cm = confusion_matrix(y_test, y_pred)
        
        # ### ä¿®æ”¹é–‹å§‹ ###ï¼šå°‡æ··æ·†çŸ©é™£æ‰“åŒ…æˆå­—å…¸ä»¥ä¾›å›å‚³
        confusion_matrix_data = {
            "matrix": cm.tolist(),  # å°‡ numpy array è½‰ç‚º python list
            "labels": target_names
        }
        print("âœ… æ··æ·†çŸ©é™£è³‡æ–™å·²ç”¢ç”Ÿã€‚")
        # ### ä¿®æ”¹çµæŸ ###

        print("\n--- åˆ†é¡å ±å‘Š ---")
        print(report)
        print("\n--- æ··æ·†çŸ©é™£ ---")
        print(f"{'':<15} | {'é æ¸¬ç‚ºéè¡Œæ˜Ÿ':<15} | {'é æ¸¬ç‚ºè¡Œæ˜Ÿ':<15}")
        print("-" * 50)
        print(f"{'å¯¦éš›ç‚ºéè¡Œæ˜Ÿ':<15} | {cm[0][0]:<15} | {cm[0][1]:<15}")
        print(f"{'å¯¦éš›ç‚ºè¡Œæ˜Ÿ':<15} | {cm[1][0]:<15} | {cm[1][1]:<15}")
        
        # --- 5. Saving Artifacts (for potential debug) and preparing for return ---
        model_dir = "temp_models"
        os.makedirs(model_dir, exist_ok=True)
        
        model_name_key = model_choice.lower()
        output_model_path = os.path.join(model_dir, f"{model_name_key}_model.joblib")
        joblib.dump(model, output_model_path)
        print(f"\nğŸ’¾ (Debug) æ¨¡å‹å‰¯æœ¬å·²å„²å­˜è‡³: {output_model_path}")
        
        trained_artifacts["model"] = model
        trained_artifacts["imputer"] = imputer
        trained_artifacts["scaler"] = scaler
        trained_artifacts["feature_names"] = feature_names
        
    except Exception as e:
        print("\n" + "="*50)
        print(f"âŒ è¨“ç·´æµç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        traceback.print_exc(file=sys.stdout)
        print("="*50)

    finally:
        sys.stdout = old_stdout

    # ### ä¿®æ”¹é–‹å§‹ ###ï¼šå›å‚³ 4 å€‹é …ç›®ï¼ŒåŒ…å«æ‰“åŒ…å¥½çš„æ··æ·†çŸ©é™£
    return output_model_path, captured_output.getvalue(), trained_artifacts, confusion_matrix_data
    # ### ä¿®æ”¹çµæŸ ###